#!/usr/bin/env python3
"""
ë§ê¸€(Ringle) ì‚¬ìš©ì í›„ê¸° í¬ë¡¤ëŸ¬
- Google Play ìŠ¤í† ì–´ ë¦¬ë·°
- í´ë¦¬ì•™ ê²Œì‹œê¸€
- ë¸”ë¼ì¸ë“œ ê³µê°œ ê²Œì‹œê¸€
- ë¸ŒëŸ°ì¹˜ ë¸”ë¡œê·¸
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import re
from datetime import datetime
import json
import os

# User-Agent ì„¤ì •
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}

# ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
all_reviews = []


def crawl_google_play():
    """Google Play ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ë§"""
    print("\n[1/4] Google Play ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ë§ ì¤‘...")

    try:
        from google_play_scraper import app, reviews_all, Sort

        # ì•± ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        app_info = app('com.ringle', lang='ko', country='kr')
        print(f"  - ì•±: {app_info['title']}")
        print(f"  - í‰ì : {app_info['score']}")
        print(f"  - ë¦¬ë·° ìˆ˜: {app_info['reviews']}")

        # ë¦¬ë·° ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 500ê°œ)
        result = reviews_all(
            'com.ringle',
            sleep_milliseconds=100,
            lang='ko',
            country='kr',
            sort=Sort.NEWEST
        )

        print(f"  - ìˆ˜ì§‘ëœ ë¦¬ë·°: {len(result)}ê°œ")

        for review in result:
            all_reviews.append({
                'platform': 'Google Play',
                'text': review['content'],
                'rating': review['score'],
                'date': review['at'].strftime('%Y-%m-%d') if review['at'] else '',
                'author': review['userName'] if review['userName'] else 'Anonymous',
                'url': f"https://play.google.com/store/apps/details?id=com.ringle&reviewId={review['reviewId']}"
            })

        print(f"  âœ… Google Play í¬ë¡¤ë§ ì™„ë£Œ: {len(result)}ê°œ")
        return len(result)

    except Exception as e:
        print(f"  âŒ Google Play í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return 0


def crawl_clien():
    """í´ë¦¬ì•™ ê²Œì‹œê¸€ í¬ë¡¤ë§"""
    print("\n[2/4] í´ë¦¬ì•™ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì¤‘...")

    clien_urls = [
        'https://www.clien.net/service/board/use/9288297',  # ë§ê¸€í”ŒëŸ¬ìŠ¤ ì‚¬ìš©ê¸°
        'https://www.clien.net/service/board/park/17774660',  # ë§ê¸€ ìˆ˜ì—… ì²´í—˜ê¸°
    ]

    count = 0
    for url in clien_urls:
        try:
            time.sleep(1)  # ìš”ì²­ ê°„ê²©
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.encoding = 'utf-8'

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'lxml')

                # ì œëª©
                title_elem = soup.select_one('.post_subject span')
                title = title_elem.get_text(strip=True) if title_elem else ''

                # ë³¸ë¬¸
                content_elem = soup.select_one('.post_article')
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for elem in content_elem.select('.attached_source, .og_box, script, style'):
                        elem.decompose()
                    text = content_elem.get_text(separator='\n', strip=True)
                else:
                    text = ''

                # ì‘ì„±ì¼
                date_elem = soup.select_one('.post_author span')
                date_str = ''
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', date_text)
                    if date_match:
                        date_str = date_match.group(1)

                # ì‘ì„±ì
                author_elem = soup.select_one('.post_info .nickname')
                author = author_elem.get_text(strip=True) if author_elem else 'Anonymous'

                if text:
                    all_reviews.append({
                        'platform': 'Clien',
                        'text': f"[{title}]\n{text}" if title else text,
                        'rating': '',
                        'date': date_str,
                        'author': author,
                        'url': url
                    })
                    count += 1
                    print(f"  - ìˆ˜ì§‘: {title[:30]}...")

        except Exception as e:
            print(f"  âš ï¸ í´ë¦¬ì•™ í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")

    print(f"  âœ… í´ë¦¬ì•™ í¬ë¡¤ë§ ì™„ë£Œ: {count}ê°œ")
    return count


def crawl_blind():
    """ë¸”ë¼ì¸ë“œ ê³µê°œ ê²Œì‹œê¸€ í¬ë¡¤ë§"""
    print("\n[3/4] ë¸”ë¼ì¸ë“œ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì¤‘...")

    # ë¸”ë¼ì¸ë“œ URLë“¤ (ê²€ìƒ‰ì—ì„œ ë°œê²¬ëœ ê²ƒë“¤)
    blind_urls = [
        'https://www.teamblind.com/kr/post/ë§ê¸€-í™”ìƒì˜ì–´-CKO0jbHJ',
        'https://www.teamblind.com/kr/post/ì˜ì–´ê³µë¶€-ë§ê¸€-ê°•ì¶”ê°•ì¶”-gfSS0fuq',
        'https://www.teamblind.com/kr/post/ë§ê¸€-ëˆê°’í•´-1HEYBaFF',
        'https://www.teamblind.com/kr/post/ë§ê¸€-ì¨ë³´ì‹ -ë¶„-J0pgRbCC',
        'https://www.teamblind.com/kr/post/ë§ê¸€-ìº ë¸”ë¦¬-ë“±-í™”ìƒ-ì˜ì–´-ì–´ë•Œ-j6QPfnxh',
        'https://www.teamblind.com/kr/post/ë§ê¸€-ì¨ë³¸ì‚¬ëŒ-ovgwdpfq',
        'https://www.teamblind.com/kr/post/ë§ê¸€-í•´-ë³¸-ì‚¬ëŒ-YZFw0ue4',
        'https://www.teamblind.com/kr/post/ë§ê¸€-í™”ìƒì˜ì–´-ê°•ì¶”-8eQePASa',
    ]

    count = 0
    for url in blind_urls:
        try:
            time.sleep(1.5)  # ìš”ì²­ ê°„ê²©
            response = requests.get(url, headers=HEADERS, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'lxml')

                # ì œëª©
                title_elem = soup.select_one('h1.title, .article-title, [class*="title"]')
                title = title_elem.get_text(strip=True) if title_elem else ''

                # ë³¸ë¬¸ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
                content_elem = soup.select_one('.article-content, .post-content, [class*="content"]')
                text = content_elem.get_text(separator='\n', strip=True) if content_elem else ''

                # JSON-LDì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œë„
                script_tags = soup.find_all('script', type='application/ld+json')
                for script in script_tags:
                    try:
                        data = json.loads(script.string)
                        if isinstance(data, dict):
                            if 'articleBody' in data:
                                text = data['articleBody']
                            if 'headline' in data:
                                title = data['headline']
                    except:
                        pass

                if text and len(text) > 20:
                    all_reviews.append({
                        'platform': 'Blind',
                        'text': f"[{title}]\n{text}" if title else text,
                        'rating': '',
                        'date': '',
                        'author': 'Anonymous',
                        'url': url
                    })
                    count += 1
                    print(f"  - ìˆ˜ì§‘: {title[:30] if title else url[-20:]}...")

        except Exception as e:
            print(f"  âš ï¸ ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    print(f"  âœ… ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ì™„ë£Œ: {count}ê°œ")
    return count


def crawl_brunch():
    """ë¸ŒëŸ°ì¹˜ ë¸”ë¡œê·¸ í¬ë¡¤ë§"""
    print("\n[4/4] ë¸ŒëŸ°ì¹˜ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì¤‘...")

    brunch_urls = [
        'https://brunch.co.kr/@0simi/162',  # 2ë…„ê°„ ë§ê¸€ 347íšŒ ìˆ˜ì—…í›„ ê¹¨ë‹¬ì€ ë§ê¸€ 200%í™œìš©ë²•
        'https://brunch.co.kr/@sunjae/21',  # ìš”ì¦˜ í‘¹ ë¹ ì§„ ë§ê¸€ Ringle ì‚¬ìš©ê¸°
        'https://brunch.co.kr/@kongkong2222/114',  # ì•„ì´ë¹„ë¦¬ê·¸ í™”ìƒì˜ì–´ ë§ê¸€ 8ê°œì›” ì§„í–‰ í›„ê¸°
    ]

    count = 0
    for url in brunch_urls:
        try:
            time.sleep(1)
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.encoding = 'utf-8'

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'lxml')

                # ì œëª©
                title_elem = soup.select_one('.cover_title, h1.title')
                title = title_elem.get_text(strip=True) if title_elem else ''

                # ë³¸ë¬¸
                content_elem = soup.select_one('.wrap_body_frame')
                if content_elem:
                    for elem in content_elem.select('script, style, .wrap_btn'):
                        elem.decompose()
                    text = content_elem.get_text(separator='\n', strip=True)
                else:
                    text = ''

                # ì‘ì„±ì¼
                date_elem = soup.select_one('.date_item, time')
                date_str = date_elem.get_text(strip=True) if date_elem else ''

                # ì‘ì„±ì
                author_elem = soup.select_one('.txt_username, .author_name')
                author = author_elem.get_text(strip=True) if author_elem else 'Anonymous'

                if text:
                    all_reviews.append({
                        'platform': 'Brunch',
                        'text': f"[{title}]\n{text}" if title else text,
                        'rating': '',
                        'date': date_str,
                        'author': author,
                        'url': url
                    })
                    count += 1
                    print(f"  - ìˆ˜ì§‘: {title[:30]}...")

        except Exception as e:
            print(f"  âš ï¸ ë¸ŒëŸ°ì¹˜ í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")

    print(f"  âœ… ë¸ŒëŸ°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ: {count}ê°œ")
    return count


def crawl_blog_reviews():
    """ê°œì¸ ë¸”ë¡œê·¸ í›„ê¸° í¬ë¡¤ë§ (ì¶”ê°€)"""
    print("\n[ì¶”ê°€] ê°œì¸ ë¸”ë¡œê·¸ í›„ê¸° í¬ë¡¤ë§ ì¤‘...")

    blog_urls = [
        ('https://kindoflegacy.com/entry/ë‚´ëˆë‚´ì‚°-ë§ê¸€-ì˜ì–´íšŒí™”-7ê°œì›”-ë¦¬ë·°-ìº ë¸”ë¦¬-ìŠ¤í”½-ë¹„êµ', 'Tistory'),
        ('https://kindoflegacy.com/entry/ë‚´ëˆë‚´ì‚°-ë§ê¸€-1ë…„-6ê°œì›”-ì†”ì§-í›„ê¸°-featìˆ˜ì—…ê°€ê²©AI-íŠœí„°ìŠ¤í„°ë””-í´ëŸ½', 'Tistory'),
    ]

    count = 0
    for url, platform in blog_urls:
        try:
            time.sleep(1)
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.encoding = 'utf-8'

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'lxml')

                # ì œëª©
                title_elem = soup.select_one('h1, .title, .entry-title')
                title = title_elem.get_text(strip=True) if title_elem else ''

                # ë³¸ë¬¸
                content_elem = soup.select_one('.entry-content, .post-content, article')
                if content_elem:
                    for elem in content_elem.select('script, style, nav, .widget'):
                        elem.decompose()
                    text = content_elem.get_text(separator='\n', strip=True)
                else:
                    text = ''

                if text and len(text) > 100:
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì €ì¥
                    if len(text) > 10000:
                        text = text[:10000] + "...(truncated)"

                    all_reviews.append({
                        'platform': platform,
                        'text': f"[{title}]\n{text}" if title else text,
                        'rating': '',
                        'date': '',
                        'author': 'Anonymous',
                        'url': url
                    })
                    count += 1
                    print(f"  - ìˆ˜ì§‘: {title[:30] if title else url[-30:]}...")

        except Exception as e:
            print(f"  âš ï¸ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    print(f"  âœ… ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì™„ë£Œ: {count}ê°œ")
    return count


def save_to_csv(output_path):
    """ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
    if not all_reviews:
        print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    df = pd.DataFrame(all_reviews)

    # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
    columns = ['platform', 'text', 'rating', 'date', 'author', 'url']
    df = df[columns]

    # í…ìŠ¤íŠ¸ ì •ë¦¬ (ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ)
    df['text'] = df['text'].apply(lambda x: ' '.join(str(x).split()) if pd.notna(x) else '')

    # CSV ì €ì¥
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\nğŸ“Š ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"   - ì´ {len(df)}ê°œ ë¦¬ë·° ìˆ˜ì§‘")
    print(f"   - í”Œë«í¼ë³„ ë¶„í¬:")
    print(df['platform'].value_counts().to_string())

    return df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ” ë§ê¸€(Ringle) ì‚¬ìš©ì í›„ê¸° í¬ë¡¤ëŸ¬")
    print("=" * 60)

    start_time = time.time()

    # ê° í”Œë«í¼ í¬ë¡¤ë§
    gp_count = crawl_google_play()
    clien_count = crawl_clien()
    blind_count = crawl_blind()
    brunch_count = crawl_brunch()
    blog_count = crawl_blog_reviews()

    # ê²°ê³¼ ì €ì¥
    output_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(output_dir, 'ringle_reviews.csv')
    df = save_to_csv(output_path)

    elapsed_time = time.time() - start_time

    print("\n" + "=" * 60)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
    print("=" * 60)

    # ìš”ì•½ í†µê³„
    print(f"\nğŸ“ˆ ìˆ˜ì§‘ ìš”ì•½:")
    print(f"   - Google Play: {gp_count}ê°œ")
    print(f"   - í´ë¦¬ì•™: {clien_count}ê°œ")
    print(f"   - ë¸”ë¼ì¸ë“œ: {blind_count}ê°œ")
    print(f"   - ë¸ŒëŸ°ì¹˜: {brunch_count}ê°œ")
    print(f"   - ë¸”ë¡œê·¸: {blog_count}ê°œ")
    print(f"   - ì´í•©: {len(all_reviews)}ê°œ")

    return df


if __name__ == '__main__':
    main()
