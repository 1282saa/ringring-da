#!/usr/bin/env python3
"""
ë§ê¸€(Ringle) ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ìˆ˜ì¡°ì‚¬ í¬ë¡¤ëŸ¬
- ë‹¤ì–‘í•œ í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰
- í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
- ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from datetime import datetime
from urllib.parse import quote, urljoin
import json
import os

# ì„¤ì •
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Referer': 'https://search.naver.com/',
}

# í‚¤ì›Œë“œ ì „ëµ
KEYWORDS = {
    'brand': [
        'ë§ê¸€',
        'Ringle',
        'ë§ê¸€í”ŒëŸ¬ìŠ¤',
        'ë§ê¸€í‹´ì¦ˆ',
    ],
    'experience': [
        'ë§ê¸€ í›„ê¸°',
        'ë§ê¸€ ë¦¬ë·°',
        'ë§ê¸€ ì‚¬ìš©í›„ê¸°',
        'ë§ê¸€ ì†”ì§í›„ê¸°',
        'ë§ê¸€ ë‚´ëˆë‚´ì‚°',
        'ë§ê¸€ ì²´í—˜',
        'ë§ê¸€ ìˆ˜ì—… í›„ê¸°',
        'ë§ê¸€ 3ê°œì›”',
        'ë§ê¸€ 6ê°œì›”',
        'ë§ê¸€ 1ë…„',
    ],
    'comparison': [
        'ë§ê¸€ vs ìº ë¸”ë¦¬',
        'ë§ê¸€ ìº ë¸”ë¦¬ ë¹„êµ',
        'ë§ê¸€ ìŠ¤í”½ ë¹„êµ',
        'ë§ê¸€ vs ìŠ¤í”½',
        'ë§ê¸€ í™”ìƒì˜ì–´ ë¹„êµ',
        'í™”ìƒì˜ì–´ ì¶”ì²œ ë§ê¸€',
    ],
    'feature': [
        'ë§ê¸€ AIíŠœí„°',
        'ë§ê¸€ íŠœí„°',
        'ë§ê¸€ ì›ì–´ë¯¼',
        'ë§ê¸€ ìˆ˜ì—…',
        'ë§ê¸€ êµì¬',
        'ë§ê¸€ í”¼ë“œë°±',
    ],
    'price': [
        'ë§ê¸€ ê°€ê²©',
        'ë§ê¸€ ìˆ˜ì—…ë£Œ',
        'ë§ê¸€ í• ì¸',
        'ë§ê¸€ ë¬´ë£Œì²´í—˜',
        'ë§ê¸€ ì¿ í°',
    ],
    'painpoint': [
        'ë§ê¸€ ë‹¨ì ',
        'ë§ê¸€ í™˜ë¶ˆ',
        'ë§ê¸€ í•´ì§€',
        'ë§ê¸€ ë¶ˆë§Œ',
    ],
}

def get_all_keywords():
    """ëª¨ë“  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    all_kw = []
    for category, keywords in KEYWORDS.items():
        all_kw.extend(keywords)
    return all_kw

def search_naver_blog(keyword, start=1, display=30):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""

    # ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰ ë¸”ë¡œê·¸ íƒ­ URL
    url = f"https://search.naver.com/search.naver"
    params = {
        'where': 'blog',
        'query': keyword,
        'start': start,
        'display': display,
    }

    try:
        response = requests.get(url, params=params, headers=HEADERS, timeout=15)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"      âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None

def parse_search_results(html):
    """ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ íŒŒì‹±"""
    results = []

    if not html:
        return results

    soup = BeautifulSoup(html, 'html.parser')

    # ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ë“¤
    items = soup.select('.view_wrap, .api_txt_lines.total_tit, li.bx')

    # ë‹¤ë¥¸ ì…€ë ‰í„°ë„ ì‹œë„
    if not items:
        items = soup.select('.total_wrap .total_group')

    if not items:
        # ê°œë³„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë§í¬ ì¶”ì¶œ
        links = soup.select('a.api_txt_lines.total_tit')
        for link in links:
            href = link.get('href', '')
            title = link.get_text(strip=True)
            if href and 'blog.naver.com' in href:
                results.append({
                    'title': title,
                    'url': href,
                    'snippet': '',
                    'author': '',
                    'date': '',
                })

    # ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹œë„
    for item in soup.select('.view_wrap'):
        try:
            # ì œëª©
            title_elem = item.select_one('.api_txt_lines.total_tit')
            title = title_elem.get_text(strip=True) if title_elem else ''

            # URL
            url = title_elem.get('href', '') if title_elem else ''

            # ìš”ì•½
            snippet_elem = item.select_one('.api_txt_lines.dsc_txt')
            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''

            # ì‘ì„±ì
            author_elem = item.select_one('.sub_txt.sub_name')
            author = author_elem.get_text(strip=True) if author_elem else ''

            # ë‚ ì§œ
            date_elem = item.select_one('.sub_txt.sub_time')
            date = date_elem.get_text(strip=True) if date_elem else ''

            if url and 'blog.naver.com' in url:
                results.append({
                    'title': title,
                    'url': url,
                    'snippet': snippet,
                    'author': author,
                    'date': date,
                })
        except Exception as e:
            continue

    return results

def get_blog_content(url):
    """ê°œë³„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        # ëª¨ë°”ì¼ URLë¡œ ë³€í™˜ (ë” ì‰¬ìš´ íŒŒì‹±)
        if 'blog.naver.com' in url:
            # iframe ë‚´ë¶€ URL ì¶”ì¶œ
            response = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            # iframe src ì°¾ê¸°
            iframe = soup.select_one('iframe#mainFrame')
            if iframe:
                iframe_src = iframe.get('src', '')
                if iframe_src:
                    if iframe_src.startswith('/'):
                        iframe_url = f"https://blog.naver.com{iframe_src}"
                    else:
                        iframe_url = iframe_src

                    response = requests.get(iframe_url, headers=HEADERS, timeout=10)
                    soup = BeautifulSoup(response.text, 'html.parser')

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_selectors = [
                '.se-main-container',
                '.post-view',
                '#postViewArea',
                '.se_component_wrap',
                '#post-area',
                '.post_ct',
            ]

            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for tag in content_elem.select('script, style, .se_oglink, .og_box'):
                        tag.decompose()

                    text = content_elem.get_text(separator=' ', strip=True)
                    if len(text) > 50:
                        return text[:8000]  # ìµœëŒ€ 8000ì

            # ì „ì²´ bodyì—ì„œ ì¶”ì¶œ ì‹œë„
            body = soup.select_one('body')
            if body:
                text = body.get_text(separator=' ', strip=True)
                if len(text) > 100:
                    return text[:8000]

        return ''

    except Exception as e:
        return ''

def crawl_keyword(keyword, max_pages=10):
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§"""
    all_results = []
    seen_urls = set()

    print(f"   ğŸ” í‚¤ì›Œë“œ: '{keyword}'")

    for page in range(1, max_pages + 1):
        start = (page - 1) * 30 + 1

        html = search_naver_blog(keyword, start=start)
        if not html:
            break

        results = parse_search_results(html)

        if not results:
            print(f"      í˜ì´ì§€ {page}: ê²°ê³¼ ì—†ìŒ, ì¢…ë£Œ")
            break

        new_count = 0
        for r in results:
            if r['url'] and r['url'] not in seen_urls:
                seen_urls.add(r['url'])
                r['search_keyword'] = keyword
                all_results.append(r)
                new_count += 1

        print(f"      í˜ì´ì§€ {page}: {new_count}ê°œ ì‹ ê·œ (ëˆ„ì : {len(all_results)}ê°œ)")

        if new_count == 0:
            break

        time.sleep(1)  # ìš”ì²­ ê°„ê²©

    return all_results

def enrich_with_content(results, sample_size=None):
    """ê²€ìƒ‰ ê²°ê³¼ì— ë³¸ë¬¸ ë‚´ìš© ì¶”ê°€"""
    print(f"\nğŸ“„ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘... (ì´ {len(results)}ê°œ)")

    if sample_size:
        results = results[:sample_size]

    for i, r in enumerate(results):
        if (i + 1) % 10 == 0:
            print(f"   ì§„í–‰: {i+1}/{len(results)}")

        content = get_blog_content(r['url'])
        r['content'] = content
        time.sleep(0.5)

    return results

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸ” ë§ê¸€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ìˆ˜ì¡°ì‚¬ í¬ë¡¤ëŸ¬")
    print("=" * 60)

    start_time = time.time()
    all_results = []
    seen_urls = set()

    keywords = get_all_keywords()
    print(f"\nğŸ“‹ ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰")
    print("-" * 60)

    for i, keyword in enumerate(keywords):
        print(f"\n[{i+1}/{len(keywords)}] ê²€ìƒ‰ ì¤‘...")
        results = crawl_keyword(keyword, max_pages=5)  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 5í˜ì´ì§€

        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
        for r in results:
            if r['url'] not in seen_urls:
                seen_urls.add(r['url'])
                all_results.append(r)

        print(f"   âœ… ì™„ë£Œ: {len(results)}ê°œ ìˆ˜ì§‘ (ì „ì²´ ëˆ„ì : {len(all_results)}ê°œ)")
        time.sleep(1)

    print("\n" + "=" * 60)
    print(f"ğŸ“Š ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(all_results)}ê°œ ê³ ìœ  í¬ìŠ¤íŠ¸")

    # ë³¸ë¬¸ ìˆ˜ì§‘ (ì „ì²´)
    print("\nğŸ“„ ë³¸ë¬¸ ë‚´ìš© ìˆ˜ì§‘ ì‹œì‘...")
    all_results = enrich_with_content(all_results)

    # DataFrame ìƒì„±
    df = pd.DataFrame(all_results)
    df['company'] = 'Ringle'
    df['source_type'] = 'blog'
    df['source_platform'] = 'Naver Blog'
    df['collected_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # ì €ì¥
    output_path = 'naver/ringle_naver_blog.csv'
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

    elapsed = time.time() - start_time

    print("\n" + "=" * 60)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   - ì´ í¬ìŠ¤íŠ¸: {len(df)}ê°œ")
    print(f"   - ë³¸ë¬¸ ìˆ˜ì§‘: {len(df[df['content'].notna() & (df['content'] != '')])}ê°œ")
    print(f"   - ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")
    print(f"   - ì €ì¥ ìœ„ì¹˜: {output_path}")
    print("=" * 60)

    # í‚¤ì›Œë“œë³„ í†µê³„
    print("\nğŸ“ˆ í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ í˜„í™©:")
    print(df['search_keyword'].value_counts().head(15).to_string())

    return df

if __name__ == '__main__':
    main()
